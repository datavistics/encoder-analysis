{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6916b743-614c-458c-8bbe-0e0b3dc4265f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53c8bd6c-c878-4ba3-99a3-274caa76e703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd2e993aeae74298822a162bc07c8918",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24205295-ad1e-4e33-9f7e-87f872d666a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET CONFIG\n",
    "DATASET='tyqiangz/multilingual-sentiments'\n",
    "MODEL='lxyuan/distilbert-base-multilingual-cased-sentiments-student'\n",
    "MIN_TOKENS=50\n",
    "MAX_TOKENS=512\n",
    "SAMPLES=10_000\n",
    "MAX_VUS = 2000\n",
    "TEXT_COLUMN='text'\n",
    "DATASET_PATH='data/dataset.json'\n",
    "K6_BIN = \"/usr/bin/k6\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d29c264-21ad-4e69-9c1a-585cf867c297",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-25 09:04:38.336\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36msrc.process_dataset\u001b[0m:\u001b[36msample_dataset\u001b[0m:\u001b[36m29\u001b[0m - \u001b[32m\u001b[1mSampled dataset down to 10000 samples\u001b[0m\n",
      "\u001b[32m2025-01-25 09:04:38.459\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36msrc.process_dataset\u001b[0m:\u001b[36msave_dataset\u001b[0m:\u001b[36m42\u001b[0m - \u001b[32m\u001b[1mSaved dataset to data/dataset.json\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from src.process_dataset import tokenize_and_filter, sample_dataset, save_dataset\n",
    "\n",
    "dataset = load_dataset(DATASET, 'all', split='train')\n",
    "\n",
    "# Map numbers to text labels\n",
    "label_mapping = {0: 'positive', 1: 'neutral', 2: 'negative'}\n",
    "dataset = dataset.map(lambda example: {'label_text': label_mapping[example['label']]})\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "dataset = tokenize_and_filter(dataset, tokenizer, text_column='text', num_proc=8)\n",
    "dataset = sample_dataset(dataset, n_samples=SAMPLES, min_tokens=MIN_TOKENS, max_tokens=MAX_TOKENS, seed=42)\n",
    "save_dataset(dataset.select_columns(['text', 'label_text']), DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63763da5-eaef-4ce8-a7a9-83ffdb6911d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field, asdict\n",
    "from typing import Dict\n",
    "\n",
    "@dataclass\n",
    "class InstanceConfig:\n",
    "    repository: str\n",
    "    accelerator: str\n",
    "    instance_size: str\n",
    "    instance_type: str\n",
    "    custom_image: Dict[str, str]  # Dict of str -> str\n",
    "\n",
    "@dataclass\n",
    "class InfinityConfig:\n",
    "    INFINITY_PORT: str = \"80\"\n",
    "    INFINITY_ENGINE: str = \"torch\"\n",
    "    INFINITY_BATCH_SIZE: str = \"16\"\n",
    "    INFINITY_DTYPE: str = \"auto\"\n",
    "    INFINITY_EMBEDDING_DTYPE: str = \"float32\"\n",
    "    INFINITY_POOLING_METHOD: str = \"auto\"\n",
    "    INFINITY_COMPILE: str = 'true'\n",
    "    INFINITY_BETTERTRANSFORMER: str = 'true'\n",
    "    INFINITY_MODEL_ID: str = \"/repository\"\n",
    "\n",
    "    def to_dict(self) -> Dict[str, str]:\n",
    "        \"\"\"Converts the dataclass to a dictionary representation.\"\"\"\n",
    "        return asdict(self)\n",
    "\n",
    "@dataclass\n",
    "class ImageConfig:\n",
    "    health_route: str = \"/health\"\n",
    "    url: str = \"michaelf34/infinity:0.0.75\"\n",
    "    env: InfinityConfig = field(default_factory=InfinityConfig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf2734e8-4ac9-41cc-9654-e2bfe7b5f19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_env(batch_size: int, image_config: ImageConfig) -> Dict[str, str]:\n",
    "    config = asdict(image_config)\n",
    "    config['env'][\"INFINITY_BATCH_SIZE\"] = str(batch_size)\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199632b1-8a67-4c7c-9573-7465c011cbf4",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ae77ff1-9e01-4837-8035-5d39257ef5d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mdeploy_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Deploys or updates a Hugging Face inference endpoint.\n",
       "\n",
       "This function checks if an existing inference endpoint with the specified name exists.\n",
       "- If found, it updates the endpoint with the provided instance configuration.\n",
       "- If not found, it creates a new inference endpoint with the given parameters.\n",
       "\n",
       "Once the endpoint is updated or created, it waits until the endpoint is fully ready.\n",
       "\n",
       "Args:\n",
       "    instance_config (InstanceConfig): A dataclass containing instance configuration details,\n",
       "                                      such as accelerator type, vendor, region, and instance size.\n",
       "\n",
       "Returns:\n",
       "    InferenceEndpoint: The deployed Hugging Face inference endpoint object.\n",
       "\n",
       "Raises:\n",
       "    Exception: If the endpoint creation process fails.\n",
       "\u001b[0;31mFile:\u001b[0m      /data/encoder-analysis/src/deployment.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src.deployment import deploy_endpoint\n",
    "?deploy_endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3743a9-8903-4792-b68b-f77c816070a8",
   "metadata": {},
   "source": [
    "## GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ec56d04-0eb0-4065-ac86-d310a00ee2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_config_experiment_dicts = [\n",
    "    {\n",
    "        'accelerator': 'gpu',\n",
    "        'instance_size': 'x1',\n",
    "        'instance_type': 'nvidia-t4'\n",
    "    },\n",
    "    {\n",
    "        'accelerator': 'gpu',\n",
    "        'instance_size': 'x1',\n",
    "        'instance_type': 'nvidia-l4'\n",
    "    },\n",
    "]\n",
    "instance_config_experiments = [InstanceConfig(repository=MODEL, **ic_exp_dict, custom_image=None) for ic_exp_dict in instance_config_experiment_dicts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64e8f4d4-cc7b-4414-9183-d84173ae1bf6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-25 09:08:04.054\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.deployment\u001b[0m:\u001b[36mdeploy_endpoint\u001b[0m:\u001b[36m84\u001b[0m - \u001b[1mCreating inference endpoint...\u001b[0m\n",
      "\u001b[32m2025-01-25 09:08:04.300\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.deployment\u001b[0m:\u001b[36mdeploy_endpoint\u001b[0m:\u001b[36m105\u001b[0m - \u001b[1mWaiting for endpoint to be ready...\u001b[0m\n",
      "\u001b[32m2025-01-25 09:10:50.081\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36msrc.deployment\u001b[0m:\u001b[36mdeploy_endpoint\u001b[0m:\u001b[36m114\u001b[0m - \u001b[32m\u001b[1mEndpoint created successfully: hw=nvidia-t4\tbs=16\tTime taken: 2m 46.03s\u001b[0m\n",
      "\u001b[32m2025-01-25 09:10:50.082\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36moptimal_vus\u001b[0m:\u001b[36m73\u001b[0m - \u001b[1mStarting exponential search for optimal VUs\u001b[0m\n",
      "\u001b[32m2025-01-25 09:10:50.082\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36moptimal_vus\u001b[0m:\u001b[36m75\u001b[0m - \u001b[1mTesting with VUs: 32\u001b[0m\n",
      "\u001b[32m2025-01-25 09:10:50.363\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36mcall_k6\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mResults written to /data/encoder-analysis/results/nvidia-t4/nvidia-t4_16_32.json\u001b[0m\n",
      "\u001b[32m2025-01-25 09:10:50.366\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36moptimal_vus\u001b[0m:\u001b[36m78\u001b[0m - \u001b[1mThroughput for 32 VUs: 0.00 req/sec\u001b[0m\n",
      "\u001b[32m2025-01-25 09:10:50.367\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36moptimal_vus\u001b[0m:\u001b[36m75\u001b[0m - \u001b[1mTesting with VUs: 64\u001b[0m\n",
      "\u001b[32m2025-01-25 09:10:50.536\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36mcall_k6\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mResults written to /data/encoder-analysis/results/nvidia-t4/nvidia-t4_16_64.json\u001b[0m\n",
      "\u001b[32m2025-01-25 09:10:50.537\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36moptimal_vus\u001b[0m:\u001b[36m78\u001b[0m - \u001b[1mThroughput for 64 VUs: 0.00 req/sec\u001b[0m\n",
      "\u001b[32m2025-01-25 09:10:50.538\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36moptimal_vus\u001b[0m:\u001b[36m75\u001b[0m - \u001b[1mTesting with VUs: 128\u001b[0m\n",
      "\u001b[32m2025-01-25 09:10:50.694\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36mcall_k6\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mResults written to /data/encoder-analysis/results/nvidia-t4/nvidia-t4_16_128.json\u001b[0m\n",
      "\u001b[32m2025-01-25 09:10:50.695\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36moptimal_vus\u001b[0m:\u001b[36m78\u001b[0m - \u001b[1mThroughput for 128 VUs: 0.00 req/sec\u001b[0m\n",
      "\u001b[32m2025-01-25 09:10:50.695\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36moptimal_vus\u001b[0m:\u001b[36m75\u001b[0m - \u001b[1mTesting with VUs: 256\u001b[0m\n",
      "\u001b[32m2025-01-25 09:10:50.853\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36mcall_k6\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mResults written to /data/encoder-analysis/results/nvidia-t4/nvidia-t4_16_256.json\u001b[0m\n",
      "\u001b[32m2025-01-25 09:10:50.853\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36moptimal_vus\u001b[0m:\u001b[36m78\u001b[0m - \u001b[1mThroughput for 256 VUs: 0.00 req/sec\u001b[0m\n",
      "\u001b[32m2025-01-25 09:10:50.854\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36moptimal_vus\u001b[0m:\u001b[36m75\u001b[0m - \u001b[1mTesting with VUs: 512\u001b[0m\n",
      "\u001b[32m2025-01-25 09:10:51.007\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36mcall_k6\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mResults written to /data/encoder-analysis/results/nvidia-t4/nvidia-t4_16_512.json\u001b[0m\n",
      "\u001b[32m2025-01-25 09:10:51.008\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36moptimal_vus\u001b[0m:\u001b[36m78\u001b[0m - \u001b[1mThroughput for 512 VUs: 0.00 req/sec\u001b[0m\n",
      "\u001b[32m2025-01-25 09:10:51.009\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36moptimal_vus\u001b[0m:\u001b[36m75\u001b[0m - \u001b[1mTesting with VUs: 1024\u001b[0m\n",
      "\u001b[32m2025-01-25 09:10:51.167\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36mcall_k6\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mResults written to /data/encoder-analysis/results/nvidia-t4/nvidia-t4_16_1024.json\u001b[0m\n",
      "\u001b[32m2025-01-25 09:10:51.168\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36moptimal_vus\u001b[0m:\u001b[36m78\u001b[0m - \u001b[1mThroughput for 1024 VUs: 0.00 req/sec\u001b[0m\n",
      "\u001b[32m2025-01-25 09:10:51.168\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36moptimal_vus\u001b[0m:\u001b[36m88\u001b[0m - \u001b[1mReached maximum VU limit: 2000\u001b[0m\n",
      "\u001b[32m2025-01-25 09:10:51.169\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36moptimal_vus\u001b[0m:\u001b[36m90\u001b[0m - \u001b[1mStarting binary search refinement\u001b[0m\n",
      "\u001b[32m2025-01-25 09:10:51.169\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36moptimal_vus\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mTesting with VUs: 768\u001b[0m\n",
      "\u001b[32m2025-01-25 09:10:51.326\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36mcall_k6\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mResults written to /data/encoder-analysis/results/nvidia-t4/nvidia-t4_16_768.json\u001b[0m\n",
      "\u001b[32m2025-01-25 09:10:51.327\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36moptimal_vus\u001b[0m:\u001b[36m102\u001b[0m - \u001b[1mThroughput did not improve with 768 VUs\u001b[0m\n",
      "\u001b[32m2025-01-25 09:10:51.328\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36moptimal_vus\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mTesting with VUs: 639\u001b[0m\n",
      "\u001b[32m2025-01-25 09:10:51.481\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36mcall_k6\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mResults written to /data/encoder-analysis/results/nvidia-t4/nvidia-t4_16_639.json\u001b[0m\n",
      "\u001b[32m2025-01-25 09:10:51.482\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36moptimal_vus\u001b[0m:\u001b[36m102\u001b[0m - \u001b[1mThroughput did not improve with 639 VUs\u001b[0m\n",
      "\u001b[32m2025-01-25 09:10:51.483\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36moptimal_vus\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mTesting with VUs: 575\u001b[0m\n",
      "\u001b[32m2025-01-25 09:10:51.657\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36mcall_k6\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mResults written to /data/encoder-analysis/results/nvidia-t4/nvidia-t4_16_575.json\u001b[0m\n",
      "\u001b[32m2025-01-25 09:10:51.658\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36moptimal_vus\u001b[0m:\u001b[36m102\u001b[0m - \u001b[1mThroughput did not improve with 575 VUs\u001b[0m\n",
      "\u001b[32m2025-01-25 09:10:51.659\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36moptimal_vus\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mTesting with VUs: 543\u001b[0m\n",
      "\u001b[32m2025-01-25 09:10:51.851\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36mcall_k6\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mResults written to /data/encoder-analysis/results/nvidia-t4/nvidia-t4_16_543.json\u001b[0m\n",
      "\u001b[32m2025-01-25 09:10:51.852\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36moptimal_vus\u001b[0m:\u001b[36m102\u001b[0m - \u001b[1mThroughput did not improve with 543 VUs\u001b[0m\n",
      "\u001b[32m2025-01-25 09:10:51.853\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36moptimal_vus\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mTesting with VUs: 527\u001b[0m\n",
      "\u001b[32m2025-01-25 09:10:52.035\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36mcall_k6\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mResults written to /data/encoder-analysis/results/nvidia-t4/nvidia-t4_16_527.json\u001b[0m\n",
      "\u001b[32m2025-01-25 09:10:52.035\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36moptimal_vus\u001b[0m:\u001b[36m102\u001b[0m - \u001b[1mThroughput did not improve with 527 VUs\u001b[0m\n",
      "\u001b[32m2025-01-25 09:10:52.036\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36moptimal_vus\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mTesting with VUs: 519\u001b[0m\n",
      "\u001b[32m2025-01-25 09:10:52.200\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36mcall_k6\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mResults written to /data/encoder-analysis/results/nvidia-t4/nvidia-t4_16_519.json\u001b[0m\n",
      "\u001b[32m2025-01-25 09:10:52.201\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36moptimal_vus\u001b[0m:\u001b[36m102\u001b[0m - \u001b[1mThroughput did not improve with 519 VUs\u001b[0m\n",
      "\u001b[32m2025-01-25 09:10:52.202\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36moptimal_vus\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mTesting with VUs: 515\u001b[0m\n",
      "\u001b[32m2025-01-25 09:10:52.379\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36mcall_k6\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mResults written to /data/encoder-analysis/results/nvidia-t4/nvidia-t4_16_515.json\u001b[0m\n",
      "\u001b[32m2025-01-25 09:10:52.380\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36moptimal_vus\u001b[0m:\u001b[36m102\u001b[0m - \u001b[1mThroughput did not improve with 515 VUs\u001b[0m\n",
      "\u001b[32m2025-01-25 09:10:52.380\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36moptimal_vus\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mTesting with VUs: 513\u001b[0m\n",
      "\u001b[32m2025-01-25 09:10:52.534\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36mcall_k6\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mResults written to /data/encoder-analysis/results/nvidia-t4/nvidia-t4_16_513.json\u001b[0m\n",
      "\u001b[32m2025-01-25 09:10:52.536\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36moptimal_vus\u001b[0m:\u001b[36m102\u001b[0m - \u001b[1mThroughput did not improve with 513 VUs\u001b[0m\n",
      "\u001b[32m2025-01-25 09:10:52.536\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36moptimal_vus\u001b[0m:\u001b[36m105\u001b[0m - \u001b[1mOptimal VUs: 512\u001b[0m\n",
      "\u001b[32m2025-01-25 09:10:57.603\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.deployment\u001b[0m:\u001b[36mdeploy_endpoint\u001b[0m:\u001b[36m84\u001b[0m - \u001b[1mCreating inference endpoint...\u001b[0m\n",
      "\u001b[32m2025-01-25 09:10:57.759\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.deployment\u001b[0m:\u001b[36mdeploy_endpoint\u001b[0m:\u001b[36m105\u001b[0m - \u001b[1mWaiting for endpoint to be ready...\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m instance_config_experiment\u001b[38;5;241m.\u001b[39mcustom_image \u001b[38;5;241m=\u001b[39m image_config\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Deploy the endpoint\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m endpoint \u001b[38;5;241m=\u001b[39m \u001b[43mdeploy_endpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance_config_experiment\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m args_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m     25\u001b[0m     endpoint\u001b[38;5;241m=\u001b[39mendpoint,\n\u001b[1;32m     26\u001b[0m     total_requests\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m     k6_bin\u001b[38;5;241m=\u001b[39mK6_BIN\n\u001b[1;32m     31\u001b[0m )\n\u001b[1;32m     33\u001b[0m optimal_vus(max_vus\u001b[38;5;241m=\u001b[39mMAX_VUS, args_dict\u001b[38;5;241m=\u001b[39margs_dict, start_vus\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n",
      "File \u001b[0;32m/data/encoder-analysis/src/deployment.py:106\u001b[0m, in \u001b[0;36mdeploy_endpoint\u001b[0;34m(instance_config)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    105\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWaiting for endpoint to be ready...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 106\u001b[0m \u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Wait for the endpoint to be ready\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# Calculate elapsed time\u001b[39;00m\n\u001b[1;32m    109\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/miniconda/lib/python3.11/site-packages/huggingface_hub/_inference_endpoints.py:218\u001b[0m, in \u001b[0;36mInferenceEndpoint.wait\u001b[0;34m(self, timeout, refresh_every)\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InferenceEndpointTimeoutError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTimeout while waiting for Inference Endpoint to be deployed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    217\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference Endpoint is not deployed yet (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). Waiting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrefresh_every\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 218\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(refresh_every)\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfetch()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from src.k6 import call_k6, optimal_vus\n",
    "from pathlib import Path\n",
    "import copy\n",
    "from time import sleep\n",
    "\n",
    "template_file = \"classification-analysis.js.j2\"\n",
    "output_file = Path(\"./generated\").resolve()/\"classification-analysis.js\"\n",
    "\n",
    "batch_sizes = [16, 32, 64, 128, 256, 512, 1024]\n",
    "start_vus = 32\n",
    "\n",
    "for og_instance_config_experiment in instance_config_experiments:\n",
    "    for batch_size in batch_sizes:\n",
    "        # Configure Infinity Settings, pass them up to the image\n",
    "        infinty_config = InfinityConfig(INFINITY_BATCH_SIZE=str(batch_size))\n",
    "        image_config = ImageConfig(env=infinty_config)\n",
    "\n",
    "        # Add the image to the HW instance\n",
    "        instance_config_experiment = copy.deepcopy(og_instance_config_experiment)\n",
    "        instance_config_experiment.custom_image = image_config\n",
    "\n",
    "        # Deploy the endpoint\n",
    "        endpoint = deploy_endpoint(instance_config_experiment)\n",
    "\n",
    "        args_dict = dict(\n",
    "            endpoint=endpoint,\n",
    "            total_requests=10,\n",
    "            template_file=template_file,\n",
    "            output_file=output_file,\n",
    "            dataset_path=DATASET_PATH,\n",
    "            k6_bin=K6_BIN\n",
    "        )\n",
    "        \n",
    "        optimal_vus(max_vus=MAX_VUS, args_dict=args_dict, start_vus=start_vus)\n",
    "        endpoint.delete()\n",
    "        sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1b6c96-f637-4dec-b1aa-54b45be09862",
   "metadata": {},
   "source": [
    "## CPU Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "36b042d5-ea78-452d-b99e-beee3f2b1521",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_config_experiment_dicts = [\n",
    "    {\n",
    "        'accelerator': 'cpu',\n",
    "        'instance_size': 'x1',\n",
    "        'instance_type': 'intel-spr'\n",
    "    }\n",
    "]\n",
    "\n",
    "instance_config_experiments = [InstanceConfig(repository=MODEL, **ic_exp_dict, custom_image=None) for ic_exp_dict in instance_config_experiment_dicts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f5e4e1f-c775-4229-9fbb-51782290cb1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-01-25 09:29:13.563\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mDeploying with Batch Size:\t1\u001b[0m\n",
      "\u001b[32m2025-01-25 09:29:13.564\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m22\u001b[0m - \u001b[1mDeploying with Infinity Engine:\ttorch\u001b[0m\n",
      "\u001b[32m2025-01-25 09:29:13.564\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.deployment\u001b[0m:\u001b[36mdeploy_endpoint\u001b[0m:\u001b[36m84\u001b[0m - \u001b[1mCreating inference endpoint...\u001b[0m\n",
      "\u001b[32m2025-01-25 09:29:13.663\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.deployment\u001b[0m:\u001b[36mdeploy_endpoint\u001b[0m:\u001b[36m105\u001b[0m - \u001b[1mWaiting for endpoint to be ready...\u001b[0m\n",
      "\u001b[32m2025-01-25 09:30:08.950\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36msrc.deployment\u001b[0m:\u001b[36mdeploy_endpoint\u001b[0m:\u001b[36m114\u001b[0m - \u001b[32m\u001b[1mEndpoint created successfully: hw=intel-spr\tbs=1\tTime taken: 0m 55.39s\u001b[0m\n",
      "\u001b[32m2025-01-25 09:30:08.951\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36moptimal_vus\u001b[0m:\u001b[36m73\u001b[0m - \u001b[1mStarting exponential search for optimal VUs\u001b[0m\n",
      "\u001b[32m2025-01-25 09:30:08.952\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36moptimal_vus\u001b[0m:\u001b[36m75\u001b[0m - \u001b[1mTesting with VUs: 1\u001b[0m\n",
      "\u001b[32m2025-01-25 09:31:10.609\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36mcall_k6\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mResults written to /data/encoder-analysis/results/intel-spr/intel-spr_1_1.json\u001b[0m\n",
      "\u001b[32m2025-01-25 09:31:10.613\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36moptimal_vus\u001b[0m:\u001b[36m78\u001b[0m - \u001b[1mThroughput for 1 VUs: 0.60 req/sec\u001b[0m\n",
      "\u001b[32m2025-01-25 09:31:10.613\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36moptimal_vus\u001b[0m:\u001b[36m75\u001b[0m - \u001b[1mTesting with VUs: 2\u001b[0m\n",
      "\u001b[32m2025-01-25 09:32:11.907\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36mcall_k6\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mResults written to /data/encoder-analysis/results/intel-spr/intel-spr_1_2.json\u001b[0m\n",
      "\u001b[32m2025-01-25 09:32:11.910\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36moptimal_vus\u001b[0m:\u001b[36m78\u001b[0m - \u001b[1mThroughput for 2 VUs: 0.59 req/sec\u001b[0m\n",
      "\u001b[32m2025-01-25 09:32:11.911\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36moptimal_vus\u001b[0m:\u001b[36m82\u001b[0m - \u001b[1mThroughput improvement is less than 5%, stopping exponential search.\u001b[0m\n",
      "\u001b[32m2025-01-25 09:32:11.911\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36moptimal_vus\u001b[0m:\u001b[36m90\u001b[0m - \u001b[1mStarting binary search refinement\u001b[0m\n",
      "\u001b[32m2025-01-25 09:32:11.912\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36moptimal_vus\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mTesting with VUs: 1\u001b[0m\n",
      "\u001b[32m2025-01-25 09:32:11.919\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36mcall_k6\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mresults file /data/encoder-analysis/results/intel-spr/intel-spr_1_1.json already exists\u001b[0m\n",
      "\u001b[32m2025-01-25 09:32:11.921\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36moptimal_vus\u001b[0m:\u001b[36m102\u001b[0m - \u001b[1mThroughput did not improve with 1 VUs\u001b[0m\n",
      "\u001b[32m2025-01-25 09:32:11.922\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36moptimal_vus\u001b[0m:\u001b[36m105\u001b[0m - \u001b[1mOptimal VUs: 1\u001b[0m\n",
      "\u001b[32m2025-01-25 09:32:17.024\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mDeploying with Batch Size:\t1\u001b[0m\n",
      "\u001b[32m2025-01-25 09:32:17.025\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m22\u001b[0m - \u001b[1mDeploying with Infinity Engine:\toptimum\u001b[0m\n",
      "\u001b[32m2025-01-25 09:32:17.026\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.deployment\u001b[0m:\u001b[36mdeploy_endpoint\u001b[0m:\u001b[36m84\u001b[0m - \u001b[1mCreating inference endpoint...\u001b[0m\n",
      "\u001b[32m2025-01-25 09:32:17.237\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.deployment\u001b[0m:\u001b[36mdeploy_endpoint\u001b[0m:\u001b[36m105\u001b[0m - \u001b[1mWaiting for endpoint to be ready...\u001b[0m\n",
      "\u001b[32m2025-01-25 09:33:12.528\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36msrc.deployment\u001b[0m:\u001b[36mdeploy_endpoint\u001b[0m:\u001b[36m114\u001b[0m - \u001b[32m\u001b[1mEndpoint created successfully: hw=intel-spr\tbs=1\tTime taken: 0m 55.50s\u001b[0m\n",
      "\u001b[32m2025-01-25 09:33:12.529\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36moptimal_vus\u001b[0m:\u001b[36m73\u001b[0m - \u001b[1mStarting exponential search for optimal VUs\u001b[0m\n",
      "\u001b[32m2025-01-25 09:33:12.530\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36moptimal_vus\u001b[0m:\u001b[36m75\u001b[0m - \u001b[1mTesting with VUs: 1\u001b[0m\n",
      "\u001b[32m2025-01-25 09:33:12.537\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36mcall_k6\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mresults file /data/encoder-analysis/results/intel-spr/intel-spr_1_1.json already exists\u001b[0m\n",
      "\u001b[32m2025-01-25 09:33:12.540\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36moptimal_vus\u001b[0m:\u001b[36m78\u001b[0m - \u001b[1mThroughput for 1 VUs: 0.60 req/sec\u001b[0m\n",
      "\u001b[32m2025-01-25 09:33:12.540\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36moptimal_vus\u001b[0m:\u001b[36m75\u001b[0m - \u001b[1mTesting with VUs: 2\u001b[0m\n",
      "\u001b[32m2025-01-25 09:33:12.545\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36mcall_k6\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mresults file /data/encoder-analysis/results/intel-spr/intel-spr_1_2.json already exists\u001b[0m\n",
      "\u001b[32m2025-01-25 09:33:12.548\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36moptimal_vus\u001b[0m:\u001b[36m78\u001b[0m - \u001b[1mThroughput for 2 VUs: 0.59 req/sec\u001b[0m\n",
      "\u001b[32m2025-01-25 09:33:12.548\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36moptimal_vus\u001b[0m:\u001b[36m82\u001b[0m - \u001b[1mThroughput improvement is less than 5%, stopping exponential search.\u001b[0m\n",
      "\u001b[32m2025-01-25 09:33:12.549\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36moptimal_vus\u001b[0m:\u001b[36m90\u001b[0m - \u001b[1mStarting binary search refinement\u001b[0m\n",
      "\u001b[32m2025-01-25 09:33:12.549\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36moptimal_vus\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mTesting with VUs: 1\u001b[0m\n",
      "\u001b[32m2025-01-25 09:33:12.553\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36mcall_k6\u001b[0m:\u001b[36m24\u001b[0m - \u001b[1mresults file /data/encoder-analysis/results/intel-spr/intel-spr_1_1.json already exists\u001b[0m\n",
      "\u001b[32m2025-01-25 09:33:12.556\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36moptimal_vus\u001b[0m:\u001b[36m102\u001b[0m - \u001b[1mThroughput did not improve with 1 VUs\u001b[0m\n",
      "\u001b[32m2025-01-25 09:33:12.557\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36moptimal_vus\u001b[0m:\u001b[36m105\u001b[0m - \u001b[1mOptimal VUs: 1\u001b[0m\n",
      "\u001b[32m2025-01-25 09:33:17.596\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mDeploying with Batch Size:\t2\u001b[0m\n",
      "\u001b[32m2025-01-25 09:33:17.597\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m22\u001b[0m - \u001b[1mDeploying with Infinity Engine:\ttorch\u001b[0m\n",
      "\u001b[32m2025-01-25 09:33:17.598\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.deployment\u001b[0m:\u001b[36mdeploy_endpoint\u001b[0m:\u001b[36m84\u001b[0m - \u001b[1mCreating inference endpoint...\u001b[0m\n",
      "\u001b[32m2025-01-25 09:33:17.764\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.deployment\u001b[0m:\u001b[36mdeploy_endpoint\u001b[0m:\u001b[36m105\u001b[0m - \u001b[1mWaiting for endpoint to be ready...\u001b[0m\n",
      "\u001b[32m2025-01-25 09:34:33.173\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36msrc.deployment\u001b[0m:\u001b[36mdeploy_endpoint\u001b[0m:\u001b[36m114\u001b[0m - \u001b[32m\u001b[1mEndpoint created successfully: hw=intel-spr\tbs=2\tTime taken: 1m 15.57s\u001b[0m\n",
      "\u001b[32m2025-01-25 09:34:33.174\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36moptimal_vus\u001b[0m:\u001b[36m73\u001b[0m - \u001b[1mStarting exponential search for optimal VUs\u001b[0m\n",
      "\u001b[32m2025-01-25 09:34:33.174\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36moptimal_vus\u001b[0m:\u001b[36m75\u001b[0m - \u001b[1mTesting with VUs: 1\u001b[0m\n",
      "\u001b[32m2025-01-25 09:35:33.605\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36mcall_k6\u001b[0m:\u001b[36m53\u001b[0m - \u001b[1mResults written to /data/encoder-analysis/results/intel-spr/intel-spr_2_1.json\u001b[0m\n",
      "\u001b[32m2025-01-25 09:35:33.609\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36moptimal_vus\u001b[0m:\u001b[36m78\u001b[0m - \u001b[1mThroughput for 1 VUs: 0.75 req/sec\u001b[0m\n",
      "\u001b[32m2025-01-25 09:35:33.609\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.k6\u001b[0m:\u001b[36moptimal_vus\u001b[0m:\u001b[36m75\u001b[0m - \u001b[1mTesting with VUs: 2\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 34\u001b[0m\n\u001b[1;32m     23\u001b[0m endpoint \u001b[38;5;241m=\u001b[39m deploy_endpoint(instance_config_experiment)\n\u001b[1;32m     25\u001b[0m args_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m     26\u001b[0m     endpoint\u001b[38;5;241m=\u001b[39mendpoint,\n\u001b[1;32m     27\u001b[0m     total_requests\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10_000\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m     k6_bin\u001b[38;5;241m=\u001b[39mK6_BIN\n\u001b[1;32m     32\u001b[0m )\n\u001b[0;32m---> 34\u001b[0m \u001b[43moptimal_vus\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_vus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_VUS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_vus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_vus\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m endpoint\u001b[38;5;241m.\u001b[39mdelete()\n\u001b[1;32m     36\u001b[0m sleep(\u001b[38;5;241m5\u001b[39m)\n",
      "File \u001b[0;32m/data/encoder-analysis/src/k6.py:76\u001b[0m, in \u001b[0;36moptimal_vus\u001b[0;34m(max_vus, args_dict, start_vus)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m vus \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m max_vus:\n\u001b[1;32m     75\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTesting with VUs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvus\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 76\u001b[0m     throughput \u001b[38;5;241m=\u001b[39m \u001b[43mcall_k6\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     vus_history\u001b[38;5;241m.\u001b[39mappend((vus, throughput))\n\u001b[1;32m     78\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThroughput for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvus\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m VUs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mthroughput\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m req/sec\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/data/encoder-analysis/src/k6.py:47\u001b[0m, in \u001b[0;36mcall_k6\u001b[0;34m(endpoint, vus, total_requests, template_file, output_file, dataset_path, k6_bin)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Run k6 using the generated file\u001b[39;00m\n\u001b[1;32m     46\u001b[0m K6_BIN \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(k6_bin)  \u001b[38;5;66;03m# Ensure correct path\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m process \u001b[38;5;241m=\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mK6_BIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mHF_TOKEN\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapture_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mResults written to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Read results\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/lib/python3.11/subprocess.py:550\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Popen(\u001b[38;5;241m*\u001b[39mpopenargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[1;32m    549\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 550\u001b[0m         stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    552\u001b[0m         process\u001b[38;5;241m.\u001b[39mkill()\n",
      "File \u001b[0;32m~/miniconda/lib/python3.11/subprocess.py:1209\u001b[0m, in \u001b[0;36mPopen.communicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1206\u001b[0m     endtime \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1209\u001b[0m     stdout, stderr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_communicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendtime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1211\u001b[0m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[1;32m   1212\u001b[0m     \u001b[38;5;66;03m# See the detailed comment in .wait().\u001b[39;00m\n\u001b[1;32m   1213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda/lib/python3.11/subprocess.py:2115\u001b[0m, in \u001b[0;36mPopen._communicate\u001b[0;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[1;32m   2108\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timeout(endtime, orig_timeout,\n\u001b[1;32m   2109\u001b[0m                         stdout, stderr,\n\u001b[1;32m   2110\u001b[0m                         skip_check_and_raise\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   2111\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(  \u001b[38;5;66;03m# Impossible :)\u001b[39;00m\n\u001b[1;32m   2112\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_check_timeout(..., skip_check_and_raise=True) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   2113\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfailed to raise TimeoutExpired.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 2115\u001b[0m ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timeout(endtime, orig_timeout, stdout, stderr)\n\u001b[1;32m   2118\u001b[0m \u001b[38;5;66;03m# XXX Rewrite these to use non-blocking I/O on the file\u001b[39;00m\n\u001b[1;32m   2119\u001b[0m \u001b[38;5;66;03m# objects; they are no longer using C stdio!\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/lib/python3.11/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selector\u001b[38;5;241m.\u001b[39mpoll(timeout)\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from loguru import logger\n",
    "from itertools import product\n",
    "\n",
    "batch_sizes = [1, 2, 4, 8, 16, 32, 64]\n",
    "infinity_engine = ['torch', 'optimum']\n",
    "start_vus = 1\n",
    "\n",
    "cross_product = list(product(batch_sizes, infinity_engine))\n",
    "\n",
    "for og_instance_config_experiment in instance_config_experiments:\n",
    "    for batch_size, infinity_engine in cross_product:\n",
    "        # Configure Infinity Settings, pass them up to the image\n",
    "        infinty_config = InfinityConfig(INFINITY_BATCH_SIZE=str(batch_size))\n",
    "        image_config = ImageConfig(env=infinty_config)\n",
    "\n",
    "        # Add the image to the HW instance\n",
    "        instance_config_experiment = copy.deepcopy(og_instance_config_experiment)\n",
    "        instance_config_experiment.custom_image = image_config\n",
    "\n",
    "        # Deploy the endpoint\n",
    "        logger.info(f'Deploying with Batch Size:\\t{batch_size}')\n",
    "        logger.info(f'Deploying with Infinity Engine:\\t{infinity_engine}')\n",
    "        endpoint = deploy_endpoint(instance_config_experiment)\n",
    "\n",
    "        args_dict = dict(\n",
    "            endpoint=endpoint,\n",
    "            total_requests=10_000,\n",
    "            template_file=template_file,\n",
    "            output_file=output_file,\n",
    "            dataset_path=DATASET_PATH,\n",
    "            k6_bin=K6_BIN\n",
    "        )\n",
    "        \n",
    "        optimal_vus(max_vus=MAX_VUS, args_dict=args_dict, start_vus=start_vus)\n",
    "        endpoint.delete()\n",
    "        sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4686724-a36c-48f2-8d88-77ea32e15f77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
